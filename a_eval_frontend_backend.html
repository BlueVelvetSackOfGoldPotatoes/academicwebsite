<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Gonçalo Hora de Carvalho</title>
  <link rel="stylesheet" type="text/css" href='../style/style.css'>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- Prism.js for code highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
</head>
</head>

<body>
  <div id="nav-placeholder"></div>

  <script>
    fetch('nav.html')
      .then(response => response.text())
      .then(data => {
        document.getElementById('nav-placeholder').innerHTML = data;
      })
      .catch(error => console.error('Error loading nav:', error));
  </script>

<main>
  <div class="container">
    <article id="parrots-turing-trap">
      <header><h2 style="text-align: center;">Evaluating Front-end & Back-end of Human Automation Interaction Applications A Hypothetical Benchmark </header></h2>
      <header><h4 style="text-align: center;">2025</header></h4>
        <p>
            Human Factors, Cognitive Engineering, and Human-Automation Interaction (HAI) form a trifecta, where users and technological systems of ever increasing autonomous control occupy a centre position. But with great autonomy comes great responsibility. It is in this context that we propose metrics and a benchmark framework based on known regimes in Artificial Intelligence (AI). A benchmark is a set of tests and metrics or measurements conducted on those tests or tasks. We hypothesise about possible tasks designed to assess operator-system interactions and both the front-end and back-end components of HAI applications. Here, front-end pertains to the user interface and direct interactions the user has with a system, while the back-end is composed of the underlying processes and mechanisms that support the front-end experience. By evaluating HAI systems through the proposed metrics, based on Cognitive Engineering studies of judgment and prediction, we attempt to unify many known taxonomies and design guidelines for HAI systems in a benchmark. This is facilitated by providing a structured approach to quantifying the efficacy and reliability of these systems in a formal way inspired by the recent fast developments in AI benchmarking techniques, thus, we attempt to guide designing principles towards a testable benchmark capable of reproducible results that is future-proof, general, and insightful both in the cognitive and technological stacks of any HAI application.
        </p>
    </article>
    <section id="preprint-display">
      <h2>Preprint</h2>
      <p>
        <strong>Evaluating Front-end & Back-end of Human Automation Interaction Applications A Hypothetical Benchmark</strong><br>
        Gonçalo Carvalho; 2025
      </p>
      <a href="https://arxiv.org/abs/2407.18953" target="_blank">Read Preprint</a>
    </section>
  </div>

</main>
<script>
  fetch('footer.html')
    .then(response => response.text())
    .then(data => {
      document.getElementById('footer-placeholder').innerHTML = data;
    })
    .catch(error => console.error('Error loading footer:', error));
</script>
<div id="footer-placeholder"></div>
</body>
</html>