<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Gonçalo Hora de Carvalho</title>
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  <link rel="preconnect" href="https://cdnjs.cloudflare.com">
  <link rel="stylesheet" type="text/css" href='style/style.css'>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js" defer></script>
</head>

<body>
  <div id="nav-placeholder"></div>

  <script>
    (function() {
      fetch('nav.html')
        .then(response => response.text())
        .then(data => {
          document.getElementById('nav-placeholder').innerHTML = data;
        })
        .catch(error => console.error('Error loading nav:', error));
    })();
  </script>

<main>
    <header><h1 style="text-align: center;">Blog</h1></header>        
    <article id="WIPs">
        <div class="container">
          <article id="parrots">
            <header><h2 style="text-align: center;">Parrots</h2>
              <h3 style="text-align: center;">2023</h3>
              <p>
                Model input essentially guides inference through a extremely dense dimensional data manifold learned from the training data. Prompting techniques such as Chain-of-Thought (CoT) and Tree-of-Thoughts (ToT), reduce task complexity by creating several input passes. With each additional token provided either during inference in CoT, ToT, or by human users, the model is incrementally guided through the high-dimensional space of learned representations - the transformer architecture learns data in many arbitrary levels of abstraction. Inside a multi-layer neural network (especially one built on self-attention like transformers), each successive layer ends up representing input data at a progressively more abstract level of meaning or function. These layers do not map cleanly onto classical linguistic categories (such as “words” at the bottom layer, “phrases” in the middle, and “semantics” at the top). Rather, the internal transformations discovered by the model are learned automatically based on the training objective (e.g., next-token prediction) and can end up capturing a variety of features-often mixing word meaning, syntactic structures, and even task-specific clues in a way that can seem surprising from a human perspective. These methods narrow in on the "correct" manifold by increasing the conditional probability of arriving at the "correct" answer as more context is added. By correct answer, we really mean "expected" or "acceptable" from the perspective a human reader, nothing more. 
              </p>
              <p>
                A transformer is composed of multiple layers, each of which receives inputs (token embeddings or the previous layer's outputs) and processes them through multi-head attention and feed-forward blocks. The output of each layer thus encodes the input in a slightly different “view,” becoming more context-informed and more specialized.
                This incremental transformation means that, early on, the model may learn to identify basic patterns or relationships (like common word co-occurrences). Deeper in the network, those representations become increasingly context-dependent - what a given word or phrase represents can shift based on surrounding text.
                In multi-head attention, each head learns statistical correlation on different parts or aspects of the input sequence. One head may learn to capture subject–verb agreement, another might latch onto semantic similarities, and yet another might look for discourse-level signals.
                By combining all these parallel attentions, each token at each layer gains a rich, multi-faceted representation that can capture many dimensions at once: syntax, semantics, and longer-range dependencies.
              </p>

              <p>
                The aforementioned critiques of the evaluation metrics used in assessing LLMs invites a deeper exploration of general intelligence - specifically how it can be reliably measured and observed in AI through rigorous and realistic tests that extend beyond linguistic capabilities to include broader cognitive functions. If we must define general intelligence (GI), one can use the "g factor", which refers to the ability to reason, plan, solve problems, think abstractly, and learn quickly across a wide range of domains. GI then involves higher-order cognitive processes that go beyond specific skills or knowledge domains.
              </p>
              <p>
                A critical issue that arises in analyzing the reasoning capabilities of large and opaque models like the GPT series is training-test set cross-contamination, which becomes increasingly problematic for the most advanced models. The massive training datasets used, comprising extensive portions of the internet, are often untraceable and completely anonymous to researchers outside the initial developer groups, to some extent even to the developers themselves, making replication studies impossible. The exact amount and identity of the data used to train models like GPT-3.5 or GPT-4 has not been publicly disclosed, posing a risk of rendering current benchmarking efforts meaningless due to cross-contamination.
              </p>
              <p>
                Researchers have attempted to counter the contamination problem using N-Gram Overlap as a metric for detection, by eliminating or withholding results for tests where answers were present in the training data. However, this method has been criticized. Blodgett et al. point out, for example, that such heuristic approaches to mitigating biases in NLP systems can be problematic and may not fully address the underlying challenges due to a lack of normative reasoning. The method is then limited in that it fails to consider the context in which N-Grams, which are a sequence of 'N' items (words, characters, phonemes, etc.) from a given text or speech corpus, appear and may discount synonymous or analogous text worded differently. Additionally, the decision to use a 200-character window around detected N-Grams during the training of GPT-3.5 is arbitrary and may not accurately reflect the influence of surrounding text on model learning.
              </p>
              <p>
                We believe that the utility of LLMs in academic fields may be underscored by their potential to aid research, for example, through automation, as a dynamic search engine, as a knowledge aggregator, or by simply enabling engaging discussions. But many problems ensue, namely that of verifying the correctness of generated content. One way of tackling this issue is by generating ever-more specialized datasets that target specific areas of interest within an academic field. And we suppose that exploring benchmarking of these systems is a valid compromise in evaluating LLMs. This is partly because any holistic evaluation such as a simulator would need to functionally encapsulate all known scientific data for that specific topic, or at least be capable of hierarchically modelling from first principles upward - arguably more complex problem than building an LLM. A paradox emerges: to fact-check the output of LLMs, we would have to build yet another system, the purpose of which would be the same as that of developing LLMs, leading us down the rabbit hole we were trying to escape from in the first place.
              </p>
              <p>
                If the reader finds the present argument overall contentious, then a question might help us converge - apart from temperature settings, which allows the consideration of less likely tokens, and from the bias, weights, and all other parameters that result in learned correlations and patterns during training, through what other mechanism would reasoning emerge? Is it through these parameters or through some "ghost in the shell"? And if we agree that reasoning is emergent via the parameters, in the sense that it mirrors learned correlations, how would valid and sound novel correlations come about, if not by luck? In order to claim that LLMs are capable of reasoning beyond the trained data, an experiment that answers these questions must be produced, showcasing some underlying mechanism by which the LLM (or any ANN) does reason, besides reproducing learned reasoning chains, producing spurious correlations, or lucky guesses.
              </p>
              <p>
                An easy enough experiment that highlights our main thesis in this paper is to try and attempt to convince an LLM to solve an unsolved scientific problem, such as a Millennium problem, say P vs NP. This is the question of whether every problem for which a solution can be verified quickly (in polynomial time, denoted as P) can also be solved quickly. If we begin by asking GPT to list known strides and advances in the theory and then work from those, we will see the model quickly hit a ceiling. We argue here that that's because the manifold space that concerns information related to the P vs NP problem "ran out" and the model is left extrapolating from the limited data it has been trained on, without the ability to generate truly novel insights or reason beyond its probabilistic patterns. This limitation highlights the fundamental difference between data-driven models and human creative and logical reasoning, which can, even if slowly and constrained by experience, explore uncharted territories regardless of existing data.
              </p>
            </article>
          </div>

          <div class="container">
            <article id="artificial_dualism">
              <header><h2 style="text-align: center;">Artificial Dualism</h2>
                <h3 style="text-align: center;">2023</h3>
                <p>
                  We contend in this section that interpreting LLMs outputs as an expression of reasoning instead of as the output of an arbitrary probability function is a mistake. In doing so we are attempting to address the bulk of truth statements or propositions that attribute higher-order cognitive functions to LLMs, or any existing algorithm for that manner. We refer to this topic as the Artificial Dualism Problem (ADP). The support for ADP is often in perceived unknowns in the context of Artificial Neural Networks (ANNs) - interestingly analogous to the God of the Gaps argument (GoG) - whereby a believer will ask the 'why' question until an unknown is discovered and readily place God there as the primal cause of that causal chain, e.g. the Big Bang. This is a space where arguments about properties such as sentience, consciousness, reasoning, and even self-awareness are often made about ANNs, such as GPT models, or, more famously, Google's LaMDA model, as reported in a letter by Google's engineer Blake Lemoine. When a user of an algorithm cannot believe that the cause of the output is sufficiently explained by the underlying architecture or technology they will readily attribute emergent intelligence, reasoning, or consciousness to the algorithm in order to explain the phenomena. The difference between ADP and the GoG is that in ADP we can empirically disprove this claim, while in GoG the claim is not falsifiable and therefore not something that a scientist or anyone in search of truth should be too concerned with.
                </p>
                <p>
                  We argue then, based on our ChildPlay results and the results of many others, that ADP occurs largely due to overextending and misinterpreting statements concerning unknowns as "emergent" properties of ANNs, while ignoring, understating, or misunderstanding the necessary conditions for higher-level cognitive functioning in human beings as we currently understand them.
                </p>
            </article>
          </div>

          <div class="container">
            <article id="black_boxes">
              <header><h2 style="text-align: center;">Black Boxes R NOT Pitch Black</h2>
                <h3 style="text-align: center;">2023</h3>
                <p>
                  AI models are often explained as input-output streams with a black box in the middle. They are further described mathematically by conditional probability formalisms. Whether this formalism is literal or not does not seem to us a matter of philosophical inquiry. While the fact that human behaviour has often been put into probabilistic terms as well makes the matter more difficult, it does not change the facts, namely that we can fully describe and understand empirically LLMs or, more generally, ANNs, regardless of the complexity involved. Work of this sort is generally referred to as mechanistic interpretability research. We believe this to be important, since the assumptions leading to ADP can be shown to be incorrect simply by referring to a model's architecture or to the ANN algorithm in general.
                </p>
                <p>
                  Traditional Dualism posits a separation between the mind and the body, often invoking an immaterial soul or consciousness that cannot be fully understood or reduced to physical processes. Unlike the mind-body problem, where our understanding of consciousness is limited by the inaccessibility of the brain's full inner workings - since dissecting it effectively destroys its function - ADP can be disproved because we have complete access to the algorithm and architecture of ANNs. The mechanistic transparency of these models allows us to empirically demonstrate that the perceived cognitive capacities are merely the result of sophisticated pattern recognition, devoid of any true sentience or consciousness. Thus, while traditional Dualism remains a philosophical challenge, ADP results from a misconception that can be directly refuted by looking at the very design and function of the systems in question.
                </p>
                <p>
                  In order to disprove ADP, at least weakly, one can simply compare ANNs, or in our case to be more specific, LLMs, with the relevant brain regions and processes that allow for decision-making, reasoning, and consciousness in it - the only other object of study we have. The absence of algorithmic equivalents to critical neurobiological structures and processes, such as the prefrontal cortex's role in executive functions, the thalamus's integration of sensory and cognitive signals, and the hippocampus's involvement in memory consolidation, together with the hyperconnectedness involved in the brain, should severely and fundamentally limit what claims regarding cognitive capacities one can make. Unlike the human brain, which seems to rely on recurrent and feedback loops, which have been linked to consciousness, self-awareness, and abstract reasoning, LLMs operate on feedforward architectures devoid of such looped structures and overall complexity. In fact, we know LLMs perform token prediction through conditional probability without engaging in any higher-order cognitive functions characteristic of sentient beings, regardless of their underlying capacity to encode higher-order probability dependencies (token to token, parts of sentences to other parts, paragraphs to paragraphs, etc). This structural limitation, we argue, precludes any emergence of higher-order thinking (including reasoning) within these models, reducing their training and subsequent outputs to sophisticated pattern recognition rather than genuine cognitive processing. They are, technically and theoretically, piggybacking off of the human training data which effectively is the outcome of many functioning brains that are, as far as neuroscience can tell, actually thinking in the real sense of the word.
                </p>
              </article>
            </div>
          <div class="container">
            <article id="turing_trap">
              <header><h2 style="text-align: center;">TTT - The Turing Trap</h2>
                <h3 style="text-align: center;">2023</h3>
                <p>
                  We now introduce the concept of the "Turing Trap" to describe the fallacy of anthropomorphizing cognitive capacities in ANNs simply because they can pass the Turing Test or generate human-like responses. The Turing Test, proposed by Alan Turing, suggests that if a machine can engage in conversation indistinguishably from a human, it can be considered intelligent. However, this test has been thoroughly criticised for not accounting for the underlying mechanisms that produce such responses both in machines and in humans, as detailed above and elsewhere.
                </p>
                <p>
                  The Turing Trap then, we argue, occurs when observers mistake the surface-level performance of ANNs for genuine cognitive capacities. This mistake seems to us to be driven by anthropomorphism, the human tendency to attribute human-like qualities to non-human entities. We think that one of the primary factors is the high level of fluency and apparent coherence in the text generated by LLMs which can create a powerful illusion of depth and intentionality. This illusion is strengthened by the model's ability to recall and reference information in ways that seem consistent with that of people who have memory and are capable of reasoning. This effect is exacerbated by the reinforcement learning training through human feedback, where human-like answers are further reinforced after general training by human technicians, which could be seen as overfitting on the task if that is what is being evaluated. In fact, the training data used in most of the existing LLMs is so massive that no individual person knows what is in it - so it is not surprising that one would be surprised to find that the neural network is capable of producing a certain piece of information or another. 

                <p>
                </p>
                  The cognitive biases behind the Turing Trap are further amplified by the design of LLMs themselves, which are optimized to produce outputs that are not just accurate but also contextually appropriate and engaging through reinforcement learning with human feedback. It follows that the more closely an AI's behavior mimics human-like interaction, the stronger the anthropomorphic response would be, leading to a higher chance of mistakenly believing that these models possess higher cognitive functions, when in fact, they are simply copying text, a capacity often referred to as "parroting".
                </p>
            </article>
          </div>
          <div class="container">
            <article id="language">
              <header><h2 style="text-align: center;">On Language</h2>
                <h3 style="text-align: center;">2022</h3>
                <p>Professor Noam Chomsky's shadow is long. Among many things that he has said, one has particular importance for the development of AI. Namely his assumption that humans are distinct from other animals by the use of language. This has given a special character to human language in the field of Linguistics where most see Chomsky as their godfather. Regardless, this particular argument-Chomsky or no Chomsky-has in turn affected AI research deeply. The assumption that human language has some magical property is and has always been guiding NLP funds, diverging all meaningful efforts to a failed endeavor (or so I will argue): to build a general language model by solving human language explicitly, that is, by mapping its syntactical, grammatical and probability rules.</p>
                <p>The assumption is an old one and has been receding hand in hand with religious beliefs. First, the difference between animals and humans was said to be that of the ephemeral soul, something that even modern philosophers dare attribute reason to. This argument has been receding ever since it was made. Initially, with the forbidden dissections of human bodies, Davinci and others could see that humans deep down (literally) didn't look much different than the common pig and that there was no trace of a "soul". Then, with the dissection of the brain, the argument went from bad to worst. In the 17th century, Descartes decided for no good reason that the pineal gland must be the root of the soul and that the reason that the soul could not be found is that the pineal gland was a communication device with the divine-a portal if you will. This didn't land much more credence in the decaying argument.</p>
                <p>Now in modernity, Cognitive Computational Neuroscience and its subfields have nearly taken the old centenary argument out of its misery. They have shown that neurons are probably the root of reason and thinking - for if you break enough of these you can make a person go from an adult to a child. And this by just removing a bit of the frontal lobe. If you break a bit more in the Broca's area then the person will produce nonsense speech-break a bit more and you end up with no speech! Start breaking neurons in the hypothalamus and you end up with someone stuck in a 5–10 second memory loop. Break a bit more and you'll certainly end up with a large-sized vegetable. All in all, every single cognitive capacity, such as self-awareness, reasoning, and speech, can be attributed to a part or parts of the brain since if you remove these you simply lose those capacities.</p> 
                
                <p>How is NLP trying to solve the problem of human language, and by assumption, Artificial General Intelligence? The current gold standard NLP models like GPT-3 and more recently DALL-E are impressive but so was the 1739s mechanical duck: the Canard Digerateur, built by Jacques de Vaucanson in an attempt to build artificial life. Neither of them is quite the real thing. In fact, despite the shinny complex arrays or cogs inside, they are both hollow, and frighteningly so. But just like the digestive duck got the attention of kings and queens, so does GPT-3. Chomsky's argument for distinguishing animals from humans is based on the capacity for language. The issue is that this is demonstrably false.</p>
                <p>It has been a long-held scientific belief (i.e. justified by field evidence that was replicated and reproduced) that language differs across animals by only a measure of quality and content. Bees have their dances which can communicate the distance and the sugar content of a food source as well as the potential location for housing a better colony. They can even elicit voting for the moving of the colony, or dance in alarm in the presence of predators-this is without mentioning all the potential arrays of communication that might be happening via the use of pheromones.</p>
                <p>Most insects-indeed, most animals-share these capacities for communication: they have different dances or sets of movements that mean particular things (visual communication) as well as a variety of pheromones, also used to detail particular meanings or contexts (e.g. trail, sex, and alarm pheromones). Insects and larger animals also communicate with sound: dolphins and whales have a rich language of clicks and whistles, and bats have echolocation which is mimicking a phonological loop (when you memorize a string of numbers you may repeat them in your mind-bats have an ongoing narrative of maps and symbols based on the bounces of their clicks), songbirds have names from birth that are particular sounds reserved to them by their parents and then by their "peers". Crows will name you a particular sound if they see you more than once and cats will slowly close their eyes in the presence of other cats if they mean no harm. The truth is, whether we like it or not, all animals that have a brain are capable of communicating information with each other. The information then, together with a brain, is the key to understanding language.</p>
                <p>The belief that other animals are simply automata whose actions are merely the outcome of a set of if-else statements or simple logical rules is wrong-this has been illustrated by experiment. Real-world environments are dynamic and chaotic. They require dynamic controllers capable of dealing with probability distributions in many different dimensions, capable of memory, context as well as self-awareness, and many other cognitive faculties. Although evolutionary pressures tend to minimize resource expenditure, the designs it outputs are far from following Occam's razor. This is to say that animals are not made simple by evolution, they might even be made complex: how else can the presence of a complex object like the brain be explained? Even insects must be capable of acting in complex environments where unpredictable states are the norm. Current Artificial Intelligence models and algorithms are not comparable with an ant or any other insect to any reasonable degree: an embodied agent acting in the real world-much less a horse, a monkey, or even you.</p>
                <p>I take then the human language to be emergent. It may or may not emerge from only the human brain. If a child does not learn a language before a certain development of the brain has occurred, evidence indicates that they won't ever be able to learn a natural language. Chomsky is correct in that languages have similarities-but I think these similarities are not just because of the brain, but rather because of their function which is to communicate about the real world (be it the inner mental world or the environment surrounding the speaking agent). Both human, as well as animal languages, carry information-Shannon in his fundamental paper on information theory (A Mathematical Theory of Communication By C. E. SHANNON, 1948) built the mathematical framework for analyzing information. Before him, Wittgenstein laid down the logical foundations of language and meaning (Tractatus Logico-Philosophicus, L WITTGENSTEIN, 1921). The culmination of these two treaties, I argue, is then the solution to language. Not human language, for human language is simply a subset of all languages, but any language or the superset of languages. This is equivalent to solving the cause of causes as opposed to solving any language explicitly as recent research has all but nearly exhausted itself in attempting.</p>
                <p>A framework must be defined then, where the general capacity of the brain to manipulate symbols that map to other symbols (also known as an ontology or dynamic general-purpose semantic network) is central, where symbols are arbitrary. This is simply an alternative algorithmic path to AGI which is fundamentally based on the importance of symbols and their mapping. The processing of which requires Shannon's theory and the representation of which requires Wittgenstein's theory. And still, these will only serve as a substitute to the rampant transformers used in NLP, and not very well, at least not in terms of results. Even then and regardless of the substitution, there is still a hole that needs feeling regarding the loopiness present in the frontal cortex that yields self-awareness (or so we think) as well as the importance of heuristics such as the ones enabled by sentiments in the human brain (check Damasio, et al). In the end, the overall complexity of the brain and its cognitive functioning is still far from being approximated.</p>
                <p>LaMDA and other programs (e.g. DALL-E2, GPT-3) are not sentient for this reason. Sentience, consciousness, or self-awareness are some of the terms that have been bent over and backward to try and fit them into our current algorithms. The reality is that non of the brain parts from which these characteristics arise are present in these algorithms-explicitly, implicitly, or functionally. So why and how would these language models ever have any perception whatsoever of anything if the perceiving mechanisms are not there, to begin with? The belief that they are sentient regardless is an outcome of the confusion involved in the field of Linguistics and in the field of AI by association.</p>
                <p>The belief is that language is the root of consciousness because we're humans and we're special and language is the last standing magical symbol of this special character. I am afraid this is wrong. The same argument made here regarding language is directly applicable to matters of consciousness. The difference across animal consciousness is not a binary one, but rather one of quality and content. And the reason is the same: the brain is the key to understanding cognitive phenomena-not a singular part, but the complex object in itself. All things emerge from it, some more specifically rooted to some cortex, others not so much.</p>
                <p>I leave you with a reference to Feynman's Cargo cult science as an analogy to the problem: it is the case that people have been trying to imitate language instead of understanding what language is.</p>
                <p>"In the South Seas, there is a Cargo Cult of people. During the war, they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they've arranged to make things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas-he's the controller-and they wait for the airplanes to land. They're doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn't work. No airplanes land. So I call these things Cargo Cult Science because they follow all the apparent precepts and forms of scientific investigation, but they're missing something essential because the planes don't land."</p>
            </article>
          </div>
          <div class="container">
            <article id="palantir">
              <header><h2 style="text-align: center;">Palantir</h2>
                <h3 style="text-align: center;">2026</h3>
                <p>Palantir (the company) deployments that appear to “predict” outcomes tend to work when two conditions hold: the organization maintains a stable data and process layer that represents operational entities and relationships, and the organization binds model outputs to an explicit decision workflow with authority, timing, and constraints; the modeling itself is often time-series forecasting for demand and capacity, anomaly detection for exceptions and fraud, and optimization for scheduling, routing, and allocation, with LLM-assisted automation used mainly to reduce interface friction rather than to generate forecasts from text alone. The practical mechanism behind this is the “ontology” pattern, where business objects such as beds, patients, orders, parts, staff, and work orders are defined as first-class entities and wired into applications that create alerts, approvals, and resourcing decisions; this supports blast-radius reasoning because the system can traverse relationships to answer questions like which downstream assemblies, customers, or operational queues change when an upstream constraint changes, and it supports auditability because the platform can surface the transformations and object-level data used to produce an output. In this setup, LLMs tend to be used for query translation over the ontology, tool calling over governed data, drafting tickets and plans that cite internal objects, and automating repetitive analysis steps; failure persists when the workflow contains constraints that are not encoded in the system, when the data is incomplete or has inconsistent semantics, or when the organization expects language output to substitute for operational authority and constraint handling. Reported hospital command-center style deployments illustrate the “works when tied to workflow” pattern because operational forecasting changes outcomes only when staffing governance, real-time feeds, and managerial authority exist to act on the forecast, while large public-sector programs illustrate a different binding constraint where legitimacy, procurement scrutiny, legal challenge risk, and clinician and public trust can determine adoption and data completeness, and data completeness in turn determines whether downstream operational analytics have stable inputs. Across these settings, recurring gaps that break the “seeing stone” narrative are that ontology modeling is a continuous maintenance cost because entities, relationships, permissions, and event semantics change with operations and policy; prediction without an intervention pathway becomes reporting output because forecasts do not change outcomes unless they connect to staffing rules, supply contracts, maintenance windows, routing authority, or budget constraints; vendor dependency risk concentrates in the operational layer because even if raw data export is possible, the encoded ontology, applications, and workflow logic are what users rely on; and LLM grounding reduces some failure modes by forcing outputs to reference internal objects and transformations but does not fix missing data, incorrect joins and semantics, unmodeled constraints, or incentive misalignment, which are often the dominant causes of non-adoption or ineffective action.</p>
            </article>
          </div>
          <div class="container">
            <article id="crystal_ball">
              <header><h2 style="text-align: center;">Crystal Ball</h2>
                <h3 style="text-align: center;">2026</h3>
                <p>Before any real work can begin, a PhD is essentially an exercise in building an accurate map of prior reality: what has been done, how it was done, when it happened, by whom, under what assumptions, with what data and instruments, and with what outcomes and failure modes. The bottleneck is not intelligence but navigation—finding the relevant fragments across disparate silos, disambiguating names and methods in papers, policy, and industry, tracing lineages of ideas across citations and market adoptions, and separating widely repeated claims from evidence-backed results.</p>

                <p>A system that makes this collective knowledge corpus machine-readable—converting academic papers, medical trials, government reports, and industrial filings into micro-graphs of experiments and claims—turns a year-long survey into a reproducible query. You can ask for the exact chain of methods and results that led to today's consensus, see who originated a technique, watch how it propagated, and pull the precise evidence spans that justify each link.</p>
                
                <p>This precondition is universal. Governments and international organizations like the UN need this map for policy and law (what interventions were tried, where, by whom, with what measurable effects). The medical establishment needs it to track clinical outcomes against theoretical claims. Companies and markets need it for business and economic decisions (what strategies worked historically, in which contexts, and which signals preceded inflection points). In all cases, the core question is identical: not what do we believe, but what has been done and how and when and by whom and what followed? We ingest data from academia, social institutions, and industry so new action starts from grounded knowledge rather than folklore, hype, or institutional memory.</p>
                
                <p>This is where the crystal ball fits: once that map exists, prediction stops being guesswork and becomes learning or reasoning (in-context) over time-indexed events. The system can forecast what comes next by pointing to the specific subgraphs and evidence that historically preceded the next step, then scoring the likely futures under explicit horizons and budgets. Generalized all the way, it is the same move in any domain: convert the past—from lab results to market shifts—into a structured, checkable record, then use it to make decisions about the future with traceable reasons. It is a big, beautiful graph of everything everywhere that has ever happened, allegedly, treating prediction as Bayesian updating over a time and context-indexed evidence graph: explicit priors, explicit likelihoods, explicit posteriors.</p>
                
                <p>We build one global, time-indexed graph, where each input contributes a micro-graph of actions and claims with strict evidence pointers. Our user or agent navigates this structure via lenses that can expand or collapse context across the macro level and the micro level under strict computational budgets. Node types: claim, experiment, dataset, instrument, protocol, model, outcome, population/context, policy intervention, market signal, etc. Edge types: supports, contradicts, uses-method, reuses-dataset, improves-upon, replicates, fails-to-replicate, cites-as-background, mandates, funds, etc.</p>
                
                <p>A. Paper Text Substrate (Temporal-Causal RAG Substrate): The foundational data layer encompassing paper metadata, medical trial protocols, government white papers, and industry filings. It maintains citation and reference contexts—the specific text spans surrounding references—to ensure we can always regress to the raw evidence. B. Micro-Graph (Meaning Substrate): Structured graphs extracted directly from the source text. For a paper, this means Methods and Results; for a government, it means Policy and Outcome; for Industry, it means Strategy and Metric. Every element retains provenance data including the source ID, character offsets, and a confidence score. C. Global Macro-Graph (Navigation and Prediction Substrate): The single global graph connecting all entities within the system. Nodes represent high-level entities such as authors, labs, grants, government bodies, and instrument classes. Every element is time-stamped, primarily by publication or filing date.</p>
                
                <p>We perform deterministic extraction rather than free-form summarization. This involves running multiple specialized extractors—agent instances prompted with specific goals. A Methods or Policy extractor identifies tools, protocols, or legislative interventions. An Experiment or Implementation extractor identifies arms, conditions, controls, and sample types in labs or real-world populations. A Results or Outcome extractor pulls metrics, effect sizes, and economic indicators. A Claims extractor isolates main claims with their scope and qualifiers, while a Limitations extractor identifies failure cases and threats to validity. Finally, we merge these outputs by resolving entities—linking a proprietary vendor term to a canonical instrument—and normalizing them to an ontology where possible.</p>
                
                <p>Prediction is treated as temporal graph learning over defined and labeled events, rather than simple LLM guessing. We define prediction targets as time-stamped events, such as a lab adopting an instrument class within 12 months, or a government scaling a medical intervention. We utilize weak labels derived from method mentions, grant text, and policy drafts, and combine them with strong labels from customer data like quotes won/lost or post-purchase publications.</p>
                
                <p>Micro-graphs provide capability-based features rather than just keywords. We analyze what experiments are being run, what throughput constraints are implied, which modalities appear in the "Methods" of industry and academia over time, and which lineages are being followed. We train temporal models on the macro graph, ranging from simple heuristics to temporal GNNs and time-to-event models. We distinguish our product by explaining predictions with provenance: we output the subgraph that drove the score, the micro chains that justify the semantics, and the evidence spans that ground each chain.</p>
                
                <p>We build moats that offer value even before the predictive model is fully improved. These high-value intermediate artifacts include Entity Normalization maps across academic and vendor terms, the Micro-graph Corpus of experiment and policy objects with evidence, the Typed Citation Graph, and Temporal Adoption Signals. The foundry is born.</p>
            </article>
          </div>
          <!-- <div class="container">
            <article id="palantiri">
              <header><h2 style="text-align: center;">Palantíri</h2>
                <h3 style="text-align: center;">2026</h3>
                <p>There's a particular kind of comedic horror in choosing a myth as your brand and then walking straight into the myth's warning label.</p>
                <p>In Tolkien, the palantíri are “Seeing Stones”: devices for long-distance vision and communication, made to extend perception across space (and sometimes time). They're not inherently evil—but they are inherently political: whoever controls the channel, controls the story. And crucially, they're dangerous because they don't just show you facts; they shape you through selective revelation, interpretation, and psychological pressure.</p>
                <p>Saruman thinks he's using the stone to gain strategic clarity; instead, the stone becomes the line through which Sauron can manipulate him.</p>
                <p>Denethor isn't mind-controlled in some simple way; he's shown enough truth (and enough curated despair) that he collapses into fatalism.</p>
                <p>The Stones amplify a very human failure mode: “I can finally see everything,” followed by “I can't stop looking,” followed by “I now mistake what I'm shown for the whole.”</p>
                <p>Now take that exact object—the iconic surveillance-adjacent artifact of modern fantasy—and name a real data-integration company Palantir. That isn't subtle. Even mainstream profiles of the firm note the name's origin and the obvious resonance: seeing far away, connecting disparate signals, and the ever-present temptation toward omniscience.</p>
                <p>And then the extra twist: Palantir (the company) builds platforms that help governments and militaries integrate and analyze large, heterogeneous datasets—turning scattered records into unified operational pictures. You don't have to do any conspiratorial fan-fiction to feel the Tolkien irony. The metaphor is just… literal.</p>
                <p>Tolkien, in a real letter, complained about modern industry borrowing mythic names without understanding what they imply—specifically joking about a motorbike product called “Ixion Cycles” (Ixion being punished on a perpetually revolving wheel in hell). He frames it as people sensing that old stories matter, but missing their “portent.”</p>
                <p>Peter Thiel's “self-fulfilling prophecy” is that his diagnosis of the world pushes him toward interventions that make his diagnosis more true.</p>
                <p>In an interview with Ross Douthat, Thiel leans heavily on a stagnation thesis: the claim that real technological velocity slowed after the mid-20th century, and that society increasingly substitutes screen-level novelty for world-level transformation.</p>
                <p>That story tends to produce a particular emotional logic: If we're stagnating, then politics becomes existential: everything feels zero-sum, high-stakes, “civilizational.” If politics is existential, you start tolerating extreme tools and extreme alliances because “normal governance” looks too slow. If you tolerate extreme tools, you build (or fund) systems that concentrate leverage—on the grounds that leverage is how you escape stagnation. And then—here's the loop—those concentrated systems intensify public fear, polarization, and institutional distrust… which feeds the sense that “democracy can't handle the moment,” which strengthens the appetite for concentrated leverage. That's the self-fulfilling prophecy: the cure becomes evidence for the diagnosis.</p>
                <p>Thiel is flirting with an explicitly apocalyptic register—talking about “the Antichrist” in the context of modern politics, and casting a kind of anti-growth regulatory future as the real civilizational threat (with Greta Thunberg used as a symbol of that politics in the exchange). Whether you take that literally, rhetorically, or as culture-war theater, the effect is the same: it frames the world as a battle over who gets to set the constraints on science, markets, and power.</p>
                <p>Now put that next to Palantir's name again. In Tolkien, the palantír is precisely the object you reach for when you believe the moment is too dire for ordinary perception—when you feel you must see more, faster, farther than everyone else, or you lose. The danger is that you stop asking what seeing is doing to you—and who else is in the circuit with you.</p>
                <p>And that's the final irony: the palantír doesn't only extend vision. It changes the viewer. It trains them into a posture: suspicion, urgency, strategic obsession, and the belief that survival requires privileged access to a hidden, total picture.</p>
                <p>Thiel's worldview (as it comes through in that interview) is unusually compatible with that posture: deep impatience with incrementalism, suspicion of mass politics, and a recurring sense that “we” are on the edge of something either transcendent (Mars, immortality) or catastrophic (stagnation, totalitarianism).</p>
                <p>The irony of calling his company Palantir after Tolkien's Palantír - no, the stupid naivity of it - is crazy. The company creates exactly the tech that will be used to poison and corrupt people with power just as it happen in LOTR. It's like showing a child what happens when you touch the stove by touching the stove and burning yourself, asking the child if they understood what happened, seeing that they did understand it, but then the child still goes ahead and touches the hot stove regardless. This as a self-reinforcing loop in which the anticipation of breakdown justifies exceptional informational power, and the exercise of that power accelerates the very breakdown it was meant to prevent. In other words, this is self-destructive.
                </p>
            </article>
          </div> -->
        </article>
      </main>
      <p></p>
      <div id="footer-placeholder"></div>
      <script>
        (function() {
          fetch('footer.html')
            .then(response => response.text())
            .then(data => {
              document.getElementById('footer-placeholder').innerHTML = data;
            })
            .catch(error => console.error('Error loading footer:', error));
        })();
      </script>
</body>
</html>