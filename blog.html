<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Gonçalo Hora de Carvalho</title>
  <link rel="stylesheet" type="text/css" href='style/style.css'>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- Prism.js for code highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
</head>
</head>

<body>
    <header id="banner">
        <h1>Gonçalo Hora de Carvalho</h1>
        <nav id="menu">
            <ul>
                <li>
                    <a href="index.html">WIPs</a>
                    <ul class="dropdown">
                        <li><a href="index.html#lego">LEGO Connect Language</a></li>
                        <li><a href="index.html#U"> \( U_i \)</a></li>
                        <li><a href="index.html#CEF">CEF</a></li>
                        <li><a href="index.html#Viral_Fungal">Viral & Fungal based ANNs</a></li>
                        <!-- Add other article links here -->
                    </ul>
                </li>
                <li>
                    <a href="blog.html">Blog</a>
                    <ul class="dropdown">
                        <li><a href="blog.html#language">On Language</a></li>
                        <!-- Add other blog links here -->
                    </ul>
                </li>
                <li>
                    <a href="about.html">About</a>
                </li>
            </ul>
        </nav>
      </header>

  <main>
    <article id="WIPs">
      <header><h1>Blog</h1></header>
      <article id="language"><h2>On Language</h2></article>
      <p>Professor Noam Chomsky's shadow is long. Among many things that he has said, one has particular importance for the development of AI. Namely his assumption that humans are distinct from other animals by the use of language. This has given a special character to human language in the field of Linguistics where most see Chomsky as their godfather. Regardless, this particular argument - Chomsky or no Chomsky - has in turn affected AI research deeply. The assumption that human language has some magical property is and has always been guiding NLP funds, diverging all meaningful efforts to a failed endeavor (or so I will argue): to build a general language model by solving human language explicitly, that is, by mapping its syntactical, grammatical and probability rules.</p>
      <p>The assumption is an old one and has been receding hand in hand with religious beliefs. First, the difference between animals and humans was said to be that of the ephemeral soul, something that even modern philosophers dare attribute reason to. This argument has been receding ever since it was made. Initially, with the forbidden dissections of human bodies, Davinci and others could see that humans deep down (literally) didn't look much different than the common pig and that there was no trace of a "soul". Then, with the dissection of the brain, the argument went from bad to worst. In the 17th century, Descartes decided for no good reason that the pineal gland must be the root of the soul and that the reason that the soul could not be found is that the pineal gland was a communication device with the divine - a portal if you will. This didn't land much more credence in the decaying argument.</p>
      <p>Now in modernity, Cognitive Computational Neuroscience and its subfields have nearly taken the old centenary argument out of its misery. They have shown that neurons are probably the root of reason and thinking - for if you break enough of these you can make a person go from an adult to a child. And this by just removing a bit of the frontal lobe. If you break a bit more in the Broca's area then the person will produce nonsense speech - break a bit more and you end up with no speech! Start breaking neurons in the hypothalamus and you end up with someone stuck in a 5–10 second memory loop. Break a bit more and you'll certainly end up with a large-sized vegetable. All in all, every single cognitive capacity, such as self-awareness, reasoning, and speech, can be attributed to a part or parts of the brain since if you remove these you simply lose those capacities.</p> 

      <p>How is NLP trying to solve the problem of human language, and by assumption, Artificial General Intelligence? The current gold standard NLP models like GPT-3 and more recently DALL-E are impressive but so was the 1739s mechanical duck: the Canard Digerateur, built by Jacques de Vaucanson in an attempt to build artificial life. Neither of them is quite the real thing. In fact, despite the shinny complex arrays or cogs inside, they are both hollow, and frighteningly so. But just like the digestive duck got the attention of kings and queens, so does GPT-3. Chomsky's argument for distinguishing animals from humans is based on the capacity for language. The issue is that this is demonstrably false.</p>
      <p>It has been a long-held scientific belief (i.e. justified by field evidence that was replicated and reproduced) that language differs across animals by only a measure of quality and content. Bees have their dances which can communicate the distance and the sugar content of a food source as well as the potential location for housing a better colony. They can even elicit voting for the moving of the colony, or dance in alarm in the presence of predators - this is without mentioning all the potential arrays of communication that might be happening via the use of pheromones.</p>
      <p>Most insects - indeed, most animals - share these capacities for communication: they have different dances or sets of movements that mean particular things (visual communication) as well as a variety of pheromones, also used to detail particular meanings or contexts (e.g. trail, sex, and alarm pheromones). Insects and larger animals also communicate with sound: dolphins and whales have a rich language of clicks and whistles, and bats have echolocation which is mimicking a phonological loop (when you memorize a string of numbers you may repeat them in your mind - bats have an ongoing narrative of maps and symbols based on the bounces of their clicks), songbirds have names from birth that are particular sounds reserved to them by their parents and then by their "peers". Crows will name you a particular sound if they see you more than once and cats will slowly close their eyes in the presence of other cats if they mean no harm. The truth is, whether we like it or not, all animals that have a brain are capable of communicating information with each other. The information then, together with a brain, is the key to understanding language.</p>
      <p>The belief that other animals are simply automata whose actions are merely the outcome of a set of if-else statements or simple logical rules is wrong - this has been illustrated by experiment. Real-world environments are dynamic and chaotic. They require dynamic controllers capable of dealing with probability distributions in many different dimensions, capable of memory, context as well as self-awareness, and many other cognitive faculties. Although evolutionary pressures tend to minimize resource expenditure, the designs it outputs are far from following Occam's razor. This is to say that animals are not made simple by evolution, they might even be made complex: how else can the presence of a complex object like the brain be explained? Even insects must be capable of acting in complex environments where unpredictable states are the norm. Current Artificial Intelligence models and algorithms are not comparable with an ant or any other insect to any reasonable degree: an embodied agent acting in the real world - much less a horse, a monkey, or even you.</p>
      <p>I take then the human language to be emergent. It may or may not emerge from only the human brain. If a child does not learn a language before a certain development of the brain has occurred, evidence indicates that they won't ever be able to learn a natural language. Chomsky is correct in that languages have similarities - but I think these similarities are not just because of the brain, but rather because of their function which is to communicate about the real world (be it the inner mental world or the environment surrounding the speaking agent). Both human, as well as animal languages, carry information - Shannon in his fundamental paper on information theory (A Mathematical Theory of Communication By C. E. SHANNON, 1948) built the mathematical framework for analyzing information. Before him, Wittgenstein laid down the logical foundations of language and meaning (Tractatus Logico-Philosophicus, L WITTGENSTEIN, 1921). The culmination of these two treaties, I argue, is then the solution to language. Not human language, for human language is simply a subset of all languages, but any language or the superset of languages. This is equivalent to solving the cause of causes as opposed to solving any language explicitly as recent research has all but nearly exhausted itself in attempting.</p>
      <p>A framework must be defined then, where the general capacity of the brain to manipulate symbols that map to other symbols (also known as an ontology or dynamic general-purpose semantic network) is central, where symbols are arbitrary. This is simply an alternative algorithmic path to AGI which is fundamentally based on the importance of symbols and their mapping. The processing of which requires Shannon's theory and the representation of which requires Wittgenstein's theory. And still, these will only serve as a substitute to the rampant transformers used in NLP, and not very well, at least not in terms of results. Even then and regardless of the substitution, there is still a hole that needs feeling regarding the loopiness present in the frontal cortex that yields self-awareness (or so we think) as well as the importance of heuristics such as the ones enabled by sentiments in the human brain (check Damasio, et al). In the end, the overall complexity of the brain and its cognitive functioning is still far from being approximated.</p>
      <p>LaMDA and other programs (e.g. DALL-E2, GPT-3) are not sentient for this reason. Sentience, consciousness, or self-awareness are some of the terms that have been bent over and backward to try and fit them into our current algorithms. The reality is that non of the brain parts from which these characteristics arise are present in these algorithms - explicitly, implicitly, or functionally. So why and how would these language models ever have any perception whatsoever of anything if the perceiving mechanisms are not there, to begin with? The belief that they are sentient regardless is an outcome of the confusion involved in the field of Linguistics and in the field of AI by association.</p>
      <p>The belief is that language is the root of consciousness because we're humans and we're special and language is the last standing magical symbol of this special character. I am afraid this is wrong. The same argument made here regarding language is directly applicable to matters of consciousness. The difference across animal consciousness is not a binary one, but rather one of quality and content. And the reason is the same: the brain is the key to understanding cognitive phenomena - not a singular part, but the complex object in itself. All things emerge from it, some more specifically rooted to some cortex, others not so much.</p>
      <p>I leave you with a reference to Feynman's Cargo cult science as an analogy to the problem: it is the case that people have been trying to imitate language instead of understanding what language is.</p>
      <p>"In the South Seas, there is a Cargo Cult of people. During the war, they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they've arranged to make things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas - he's the controller - and they wait for the airplanes to land. They're doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn't work. No airplanes land. So I call these things Cargo Cult Science because they follow all the apparent precepts and forms of scientific investigation, but they're missing something essential because the planes don't land."</p>
    </article>
  </main>

  <footer>
    <p><a href="#banner">Return to Top</a></p>
  </footer>
</body>
</html>